{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Reinforcement Learning using an example of a maze environment that the agent needs to explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement Reinforcement Learning (RL) in a maze environment, we can use Q-Learning, which is a model-free RL algorithm. The agent will explore the maze, take actions, and learn from the consequences of those actions to reach a goal.\n",
    "\n",
    "We'll break down the process into the following steps:\n",
    "\n",
    "Environment Setup: We’ll define a maze where the agent can move.\n",
    "Q-Learning Algorithm: The agent will learn from exploring the maze using the Q-Learning algorithm.\n",
    "Agent Movement: The agent will move based on its Q-values and explore until it finds the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Environment Setup\n",
    "# First, we define a simple maze environment. Let's assume the maze is represented as a grid of states where the agent can move in four directions: up, down, left, and right. The agent will receive rewards when it reaches the goal and negative rewards for moving into walls or invalid cells.\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the maze grid, where 0 is an empty space, 1 is a wall, and 9 is the goal.\n",
    "maze = np.array([\n",
    "    [0, 0, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 0],\n",
    "    [0, 1, 1, 1, 0, 9],\n",
    "    [0, 0, 0, 0, 0, 0]\n",
    "])\n",
    "\n",
    "# Define the action space\n",
    "# 0: Up, 1: Down, 2: Left, 3: Right\n",
    "actions = [(0, -1), (0, 1), (-1, 0), (1, 0)]  # (row, col) movements\n",
    "\n",
    "# Start and goal positions\n",
    "start = (0, 0)  # Start position at top-left corner\n",
    "goal = (3, 5)  # Goal position at bottom-right corner\n",
    "\n",
    "# Create a simple environment class\n",
    "class MazeEnv:\n",
    "    def __init__(self, maze, start, goal):\n",
    "        self.maze = maze\n",
    "        self.start = start\n",
    "        self.goal = goal\n",
    "        self.agent_position = start\n",
    "        \n",
    "    def reset(self):\n",
    "        self.agent_position = self.start\n",
    "        return self.agent_position\n",
    "\n",
    "    def step(self, action):\n",
    "        # Get the new position after taking the action\n",
    "        move = actions[action]\n",
    "        new_position = (self.agent_position[0] + move[0], self.agent_position[1] + move[1])\n",
    "        \n",
    "        # Check for boundaries or walls\n",
    "        if (0 <= new_position[0] < self.maze.shape[0] and 0 <= new_position[1] < self.maze.shape[1] and \n",
    "            self.maze[new_position] != 1):\n",
    "            self.agent_position = new_position\n",
    "        \n",
    "        # Check if goal is reached\n",
    "        if self.agent_position == self.goal:\n",
    "            return self.agent_position, 10, True  # 10 reward for reaching the goal\n",
    "        \n",
    "        return self.agent_position, -1, False  # -1 penalty for each step\n",
    "        \n",
    "    def render(self):\n",
    "        # Visualize the agent's position in the maze\n",
    "        maze_copy = np.copy(self.maze)\n",
    "        maze_copy[self.agent_position] = 2  # Mark agent's position\n",
    "        print(maze_copy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Q-Learning Algorithm\n",
    "# Now, we’ll implement the Q-Learning algorithm, which involves updating Q-values based on the agent's experiences. The agent learns the best actions to take by receiving feedback from the environment.\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, learning_rate=0.1, discount_factor=0.9, exploration_rate=1.0, exploration_decay=0.995):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_decay = exploration_decay\n",
    "        self.q_table = np.zeros((env.maze.shape[0], env.maze.shape[1], len(actions)))  # Q-value table\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.uniform(0, 1) < self.exploration_rate:\n",
    "            # Exploration: choose a random action\n",
    "            return random.choice(range(len(actions)))\n",
    "        else:\n",
    "            # Exploitation: choose the best action based on Q-values\n",
    "            return np.argmax(self.q_table[state[0], state[1], :])\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        # Get the best future Q-value for the next state\n",
    "        future_q = np.max(self.q_table[next_state[0], next_state[1], :])\n",
    "        \n",
    "        # Update Q-value for the current state-action pair\n",
    "        self.q_table[state[0], state[1], action] = (1 - self.learning_rate) * self.q_table[state[0], state[1], action] + \\\n",
    "                                                      self.learning_rate * (reward + self.discount_factor * future_q)\n",
    "\n",
    "    def train(self, episodes=1000):\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            while not done:\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done = self.env.step(action)\n",
    "                self.learn(state, action, reward, next_state)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                \n",
    "            # Decay the exploration rate\n",
    "            self.exploration_rate *= self.exploration_decay\n",
    "            if episode % 100 == 0:\n",
    "                print(f\"Episode {episode}, Total Reward: {total_reward}, Exploration Rate: {self.exploration_rate}\")\n",
    "\n",
    "    def test(self):\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action = self.choose_action(state)\n",
    "            next_state, reward, done = self.env.step(action)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            self.env.render()\n",
    "            if done:\n",
    "                print(f\"Goal reached with total reward: {total_reward}\")\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -181, Exploration Rate: 0.995\n",
      "Episode 100, Total Reward: -5, Exploration Rate: 0.6027415843082742\n",
      "Episode 200, Total Reward: 0, Exploration Rate: 0.36512303261753626\n",
      "Episode 300, Total Reward: -1, Exploration Rate: 0.2211807388415433\n",
      "Episode 400, Total Reward: 1, Exploration Rate: 0.13398475271138335\n",
      "Episode 500, Total Reward: 2, Exploration Rate: 0.0811640021330769\n",
      "Episode 600, Total Reward: 1, Exploration Rate: 0.04916675299948831\n",
      "Episode 700, Total Reward: 1, Exploration Rate: 0.029783765425331846\n",
      "Episode 800, Total Reward: 1, Exploration Rate: 0.018042124582040707\n",
      "Episode 900, Total Reward: 3, Exploration Rate: 0.010929385683282892\n",
      "Testing the trained agent:\n",
      "[[0 2 0 1 0 0]\n",
      " [0 1 0 1 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 1 1 1 0 9]\n",
      " [0 0 0 0 0 0]]\n",
      "[[0 0 2 1 0 0]\n",
      " [0 1 0 1 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 1 1 1 0 9]\n",
      " [0 0 0 0 0 0]]\n",
      "[[0 0 0 1 0 0]\n",
      " [0 1 2 1 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 1 1 1 0 9]\n",
      " [0 0 0 0 0 0]]\n",
      "[[0 0 0 1 0 0]\n",
      " [0 1 0 1 0 0]\n",
      " [0 1 2 0 0 0]\n",
      " [0 1 1 1 0 9]\n",
      " [0 0 0 0 0 0]]\n",
      "[[0 0 0 1 0 0]\n",
      " [0 1 0 1 0 0]\n",
      " [0 1 0 2 0 0]\n",
      " [0 1 1 1 0 9]\n",
      " [0 0 0 0 0 0]]\n",
      "[[0 0 0 1 0 0]\n",
      " [0 1 0 1 0 0]\n",
      " [0 1 0 0 2 0]\n",
      " [0 1 1 1 0 9]\n",
      " [0 0 0 0 0 0]]\n",
      "[[0 0 0 1 0 0]\n",
      " [0 1 0 1 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 1 1 1 2 9]\n",
      " [0 0 0 0 0 0]]\n",
      "[[0 0 0 1 0 0]\n",
      " [0 1 0 1 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 1 1 1 0 2]\n",
      " [0 0 0 0 0 0]]\n",
      "Goal reached with total reward: 3\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Training the Agent\n",
    "# Now, let’s create an instance of the environment and the QLearning agent, train the agent, and then test it.\n",
    "\n",
    "# Create the environment and agent\n",
    "env = MazeEnv(maze, start, goal)\n",
    "agent = QLearningAgent(env)\n",
    "\n",
    "# Train the agent\n",
    "agent.train(episodes=1000)\n",
    "\n",
    "# Test the agent by visualizing the path to the goal\n",
    "print(\"Testing the trained agent:\")\n",
    "agent.test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key Points:\n",
    "\n",
    "# State Representation: The state is represented by the agent's position in the maze (a pair of row, column).\n",
    "# Action Space: The agent can choose from four possible actions (up, down, left, right).\n",
    "# Q-Table: A table that holds Q-values for every state-action pair.\n",
    "# Exploration vs. Exploitation: The agent explores the environment at first (with random actions), but as it learns, it starts exploiting the knowledge it has acquired (choosing actions with the highest Q-values).\n",
    "\n",
    "\n",
    "# How It Works:\n",
    "\n",
    "# The agent starts at the top-left corner of the maze (start = (0, 0)).\n",
    "# It takes actions in the maze based on its Q-values.\n",
    "# When the agent reaches the goal, it gets a positive reward (10 points).\n",
    "# The agent learns the best actions to take in the maze by updating its Q-values based on the rewards it receives.\n",
    "# Over time, the agent becomes better at navigating the maze, moving towards the goal in the fewest steps possible.\n",
    "\n",
    "# Output:\n",
    "\n",
    "# The agent will explore the maze and update its knowledge. After training for several episodes, the agent will learn the optimal path to the goal. You will see the Q-values being updated during training, and after training, the agent will demonstrate its learned policy by navigating to the goal.\n",
    "\n",
    "# This is a basic implementation of Q-learning in a maze environment. Let me know if you'd like to enhance or further customize the implementation!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
